{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will attempt to classify text messages as \"SPAM\" or \"HAM\" using TF-IDF Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "df_messages = pd.read_csv('spam.csv', usecols=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert string labels to 1 or 0 \n",
    "le = LabelEncoder()\n",
    "df_messages['target'] = le.fit_transform(df_messages['v1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that takes in a string and returns a list of tokens or words \n",
    "def tokenize(string):\n",
    "    '''\n",
    "    Tokenizes a string\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    string: str object\n",
    "        String object to tokenize\n",
    "    Returns\n",
    "    --------\n",
    "    tokens : list\n",
    "        A list containing each word in string as an element \n",
    "\n",
    "    '''\n",
    "    \n",
    "    tokens = word_tokenize(string)\n",
    "    return tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to remove stopwords and punctuation from a list of tokens\n",
    "def remove_stopwords(tokens): \n",
    "    '''\n",
    "    Removes stopwords from a list of tokens (words)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: list object\n",
    "        List of tokens that need stopwords removed\n",
    "    Returns\n",
    "    --------\n",
    "    stopwords_removed : list object\n",
    "        A list containing tokens with stopwords removed\n",
    "\n",
    "    '''\n",
    "    stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "    stopwords_removed = [token.lower().replace('ï¿½','') for token in tokens if token not in stopwords_list]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply tokenization and stop word removal to our dataset \n",
    "df_messages['tokens'] = df_messages['v2'].apply(lambda x: tokenize(x))\n",
    "df_messages['stopwords_removed'] = df_messages['tokens'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that outputs the frequency of the n most common words\n",
    "def frequency_distribution(tokens, n):\n",
    "    '''\n",
    "    Get n most common words in a Series of tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: pandas.Series object\n",
    "        Pandas series of tokens \n",
    "    n : int object\n",
    "        Integer defining the number of most common words to return\n",
    "    Returns\n",
    "    --------\n",
    "    most_common : list object\n",
    "        An array of tuples containing word frequency for n most common words\n",
    "\n",
    "    '''\n",
    "    messages_concat = []\n",
    "    for message in tokens:\n",
    "        messages_concat += message\n",
    "    messages_freqdist = FreqDist(messages_concat)\n",
    "    most_common = messages_freqdist.most_common(n)\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 1952),\n",
       " ('...', 1233),\n",
       " ('u', 1118),\n",
       " ('call', 576),\n",
       " (\"'s\", 492),\n",
       " ('2', 485),\n",
       " (\"'m\", 395),\n",
       " ('get', 385),\n",
       " ('ur', 381),\n",
       " (\"n't\", 361)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_distribution(df_messages['stopwords_removed'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate tf-idf vectorization for our data (split data into train and test here)\n",
    "def tfidf(X, y): \n",
    "    '''\n",
    "    Generate train and test TF-IDF vectorization for our data set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pandas.Series object\n",
    "        Pandas series of text documents to classify \n",
    "    y : pandas.Series object\n",
    "        Pandas series containing label for each document\n",
    "    Returns\n",
    "    --------\n",
    "    tf_idf_train :  sparse matrix, [n_train_samples, n_features]\n",
    "        Vector representation of train data\n",
    "    tf_idf_test :  sparse matrix, [n_test_samples, n_features]\n",
    "        Vector representation of test data\n",
    "\n",
    "    '''\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords_list)\n",
    "\n",
    "    tf_idf_train = vectorizer.fit_transform(X_train)\n",
    "    tf_idf_test = vectorizer.transform(X_test)\n",
    "    return tf_idf_train, tf_idf_test, y_train, y_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train, tf_idf_test, y_train, y_test, vecotorizer = tfidf(df_messages['v2'], df_messages['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that takes in a classifier and trains it on our tf-idf vectors and generates test and train predictiions\n",
    "def classify_text(classifier, tf_idf_train, tf_idf_test, y_train):\n",
    "    '''\n",
    "    Train a classifier to identify whether a message is spam or ham\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    classifier: sklearn classifier\n",
    "       initialized sklearn classifier (MultinomialNB, RandomForestClassifier, etc.)\n",
    "    tf_idf_train : sparse matrix, [n_train_samples, n_features]\n",
    "        TF-IDF vectorization of train data\n",
    "    tf_idf_test : sparse matrix, [n_test_samples, n_features]\n",
    "        TF-IDF vectorization of test data\n",
    "    y_train : pandas.Series object\n",
    "        Pandas series containing label for each document in the train set\n",
    "    Returns\n",
    "    --------\n",
    "    train_preds :  list object\n",
    "        Predictions for train data\n",
    "    test_preds :  list object\n",
    "        Predictions for test data\n",
    "    '''\n",
    "    classifier.fit(tf_idf_train, y_train)\n",
    "    train_preds = classifier.predict(tf_idf_train)\n",
    "    test_preds = classifier.predict(tf_idf_test)\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate predictions for Naive Bayes Classifier\n",
    "nb_train_preds, nb_test_preds = classify_text(nb_classifier,tf_idf_train, tf_idf_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1202    0]\n",
      " [  44  147]]\n",
      "0.968413496051687\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, nb_test_preds))\n",
    "print(accuracy_score(y_test, nb_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate predictions for Random Forest Classifier\n",
    "rf_train_preds, rf_test_preds = classify_text(rf_classifier,tf_idf_train, tf_idf_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1202    0]\n",
      " [  34  157]]\n",
      "0.9755922469490309\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, rf_test_preds))\n",
    "print(accuracy_score(y_test, rf_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_tf_idf(tf_idf, doc):\n",
    "    '''\n",
    "    Get word with highest TF-IDF value in a document\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tf_idf : spare matrix \n",
    "        TF-IDF vectorization of text data\n",
    "    doc : int object\n",
    "        Index of document in vectorization to get max tf-idf for\n",
    "    --------\n",
    "    max_tf_idf : str object\n",
    "        Word with highest TF-IDF value in a document\n",
    "    '''\n",
    "    x = tf_idf[doc].toarray()\n",
    "    max_tf_idf = vecotorizer.get_feature_names()[np.where(x[0] == max(x[0]))[0][0]]\n",
    "    print(max_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain\n",
    "The word schools has the highest TF-IDF value in the second document of our test data. What does that tell us about the word school? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schools\n"
     ]
    }
   ],
   "source": [
    "get_max_tf_idf(tf_idf_test, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
